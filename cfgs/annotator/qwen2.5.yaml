# Qwen2.5-32B
name: qwen2.5  # the name of the configuration
chat: False
checkpoint: model/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4  # your path to the model checkpoint for local inference
# qwen has system prompt. We can input the examples in a form of chatting
# https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
anno_temperature: 0.5  # temperature for this model.
anno_top_p: 0.8  # top_p for this model. The smaller the value, the more deterministic the model output is.
anno_max_tokens: 100  # maximum number of tokens to generate per output sequence.
repetition_penalty: 1  # set to 1 to avoid repetition penalty
anno_bs: 5  # batch size for this model
dtype: half  # https://docs.vllm.ai/en/latest/models/engine_args.html#cmdoption-dtype
gpu_memory_utilization: 0.7
stream: False  # if True, the model will be used in stream mode. This is useful for chat models.
tensor_parallel_size: 2
enable_chunked_prefill: False  # https://github.com/vllm-project/vllm/issues/6723, set explicitly enable_chunked_prefill to False For Volta GPU
